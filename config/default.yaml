# Default configuration for LLM Gateway
server:
  port: 8080
  host: "0.0.0.0"
  timeout: 30000
  keepAliveTimeout: 5000
  headersTimeout: 6000
  corsEnabled: true
  corsOrigins: "*"
  rateLimitingEnabled: true
  ssl:
    enabled: false
    keyPath: ""
    certPath: ""

auth:
  mode: "hybrid"  # gateway, client, hybrid
  allowClientKeys: true
  requireAuthHeader: false
  apiKeyHeader: "X-API-Key"

providers:
  openai:
    enabled: true
    baseUrl: "https://api.openai.com/v1"
    timeout: 30000
    retryCount: 3
    retryDelay: 1000
    useResponsesAPI: true  # Use responses API by default, set to false for chat completions
    features:
      responses_api: true
      background_processing: true
      built_in_tools: true
      streaming_events: true
    models:
      # GPT-4o Series
      gpt-4o:
        costPerInputToken: 0.0000025
        costPerOutputToken: 0.00001
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "audio"]
      gpt-4o-audio:
        costPerInputToken: 0.0000025
        costPerOutputToken: 0.00001
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "audio", "realtime"]
      gpt-4o-realtime:
        costPerInputToken: 0.0000025
        costPerOutputToken: 0.00001
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "audio", "realtime"]
      gpt-4o-search-preview:
        costPerInputToken: 0.0000025
        costPerOutputToken: 0.00001
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "web_search"]
      gpt-4o-transcribe:
        costPerInputToken: 0.000012
        costPerOutputToken: 0
        maxTokens: null
        type: "transcription"
        features: ["transcription", "streaming", "logprobs"]
      gpt-4o-mini:
        costPerInputToken: 0.00000015
        costPerOutputToken: 0.0000006
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision"]
      gpt-4o-mini-audio:
        costPerInputToken: 0.00000015
        costPerOutputToken: 0.0000006
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "audio", "realtime"]
      gpt-4o-mini-realtime:
        costPerInputToken: 0.00000015
        costPerOutputToken: 0.0000006
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "audio", "realtime"]
      gpt-4o-mini-search-preview:
        costPerInputToken: 0.00000015
        costPerOutputToken: 0.0000006
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision", "web_search"]
      gpt-4o-mini-transcribe:
        costPerInputToken: 0.000008
        costPerOutputToken: 0
        maxTokens: null
        type: "transcription"
        features: ["transcription", "streaming", "logprobs"]
      gpt-4o-mini-tts:
        costPerInputToken: 0.000025
        costPerOutputToken: 0
        maxTokens: 4096
        type: "tts"
        features: ["tts", "streaming", "voice_instructions"]
      
      # GPT-4 Series
      gpt-4-turbo:
        costPerInputToken: 0.00001
        costPerOutputToken: 0.00003
        maxTokens: 128000
        multimodal: true
        features: ["completion", "streaming", "multimodal", "tools", "vision"]
      gpt-4.1:
        costPerInputToken: 0.00001
        costPerOutputToken: 0.00003
        maxTokens: 128000
        features: ["completion", "streaming", "tools"]
      gpt-4:
        costPerInputToken: 0.00003
        costPerOutputToken: 0.00006
        maxTokens: 8192
        features: ["completion", "streaming", "tools"]
        
      # O-Series (Reasoning Models)
      o3:
        costPerInputToken: 0.000005
        costPerOutputToken: 0.000015
        maxTokens: 200000
        features: ["completion", "reasoning", "tools"]
      o3-pro:
        costPerInputToken: 0.000015
        costPerOutputToken: 0.00005
        maxTokens: 200000
        features: ["completion", "reasoning", "tools", "advanced_reasoning"]
      o3-mini:
        costPerInputToken: 0.000001
        costPerOutputToken: 0.000004
        maxTokens: 128000
        features: ["completion", "reasoning", "tools"]
      o3-deep-research:
        costPerInputToken: 0.00002
        costPerOutputToken: 0.00008
        maxTokens: 200000
        features: ["completion", "reasoning", "tools", "deep_research"]
      o4-mini:
        costPerInputToken: 0.0000008
        costPerOutputToken: 0.0000032
        maxTokens: 128000
        features: ["completion", "reasoning", "tools"]
      o4-mini-deep-research:
        costPerInputToken: 0.000008
        costPerOutputToken: 0.000032
        maxTokens: 128000
        features: ["completion", "reasoning", "tools", "deep_research"]
      o1-preview:
        costPerInputToken: 0.000015
        costPerOutputToken: 0.00006
        maxTokens: 128000
        features: ["completion", "reasoning"]
      o1-mini:
        costPerInputToken: 0.000003
        costPerOutputToken: 0.000012
        maxTokens: 128000
        features: ["completion", "reasoning"]
        
      # Audio Models
      whisper-1:
        costPerInputToken: 0.006
        costPerOutputToken: 0
        maxTokens: null
        type: "transcription"
        unit: "per_minute"
        features: ["transcription", "translation"]
      tts-1:
        costPerInputToken: 0.000015
        costPerOutputToken: 0
        maxTokens: 4096
        type: "tts"
        unit: "per_1M_characters"
        features: ["tts"]
      tts-1-hd:
        costPerInputToken: 0.00003
        costPerOutputToken: 0
        maxTokens: 4096
        type: "tts"
        unit: "per_1M_characters"
        features: ["tts", "high_definition"]
        
      # Embedding Models
      text-embedding-3-large:
        costPerInputToken: 0.00000013
        costPerOutputToken: 0
        maxTokens: 8191
        type: "embedding"
        dimensions: 3072
        features: ["embedding", "reduced_dimensions"]
      text-embedding-3-small:
        costPerInputToken: 0.00000002
        costPerOutputToken: 0
        maxTokens: 8191
        type: "embedding"
        dimensions: 1536
        features: ["embedding", "reduced_dimensions"]
      text-embedding-ada-002:
        costPerInputToken: 0.0000001
        costPerOutputToken: 0
        maxTokens: 8191
        type: "embedding"
        dimensions: 1536
        features: ["embedding"]
      
  gemini:
    enabled: true
    baseUrl: "https://generativelanguage.googleapis.com/v1beta"
    timeout: 30000
    retryCount: 3
    retryDelay: 1000
    models:
      gemini-2.5-pro:
        costPerInputToken: 0.00000125
        costPerOutputToken: 0.00000375
        maxTokens: 1000000
        multimodal: true
      gemini-2.5-flash:
        costPerInputToken: 0.000000075
        costPerOutputToken: 0.0000003
        maxTokens: 1000000
        multimodal: true
      gemini-2.0-flash:
        costPerInputToken: 0.000000075
        costPerOutputToken: 0.0000003
        maxTokens: 1000000
        multimodal: true

routing:
  strategy: "cost_optimized"  # round_robin, performance, cost_optimized, health_based
  failoverEnabled: false
  healthCheckInterval: 900000 # 15 minutes
  circuitBreakerThreshold: 5
  circuitBreakerTimeout: 60000

cache:
  enabled: true
  backend: "memory"  # memory, redis
  ttl: 3600
  maxSize: 1000
  redis:
    url: "redis://localhost:6379"
    keyPrefix: "llm_gateway:"
    db: 0

rateLimit:
  # Global default rate limiting (very relaxed for self-hosted)
  windowMs: 900000  # 15 minutes
  max: 10000  # 10,000 requests per 15 minutes (very generous)
  message: "Too many requests, please try again later"
  standardHeaders: true
  legacyHeaders: false
  
  # Endpoint-specific rate limits (much more generous)
  chat:
    strategy: "token-bucket"
    capacity: 5000  # Allow 5000 token burst
    refillRate: 1000  # Refill 1000 tokens per minute
    refillPeriod: 60000  # 1 minute
    message: "Too many chat requests, please slow down"
  
  embeddings:
    windowMs: 60000  # 1 minute
    max: 1000  # 1000 requests per minute
    message: "Too many embedding requests, please slow down"
  
  audio:
    windowMs: 300000  # 5 minutes  
    max: 500  # 500 requests per 5 minutes
    message: "Too many audio requests, please slow down"
  
  models:
    windowMs: 1800000  # 30 minutes
    max: 1  # 1 request per 30 minutes (internal checks only)
    message: "Model listing is for internal checks only - limit: 1 per 30 minutes"
  
  health:
    windowMs: 1800000  # 30 minutes  
    max: 1  # 1 request per 30 minutes (internal checks only)
    message: "Health checks are for internal monitoring only - limit: 1 per 30 minutes"

logging:
  level: "info"  # error, warn, info, debug
  format: "json"  # json, text
  correlationId: true
  requestLogging: true
  errorLogging: true

metrics:
  enabled: true
  endpoint: "/metrics"
  collectDefaultMetrics: true
  requestDuration: true
  providerMetrics: true

security:
  helmet:
    contentSecurityPolicy: false
  cors:
    credentials: true
  requestSizeLimit: "10mb"

# Realtime feature configuration (gateway-level)
realtime:
  enabled: true
  models:
    # OpenAI Realtime Models
    - id: gpt-4o-realtime-preview-2025-06-03
      provider: openai
      input:
        sample_rate_hz: 24000
        mime_type: "audio/pcm;rate=24000"
      vad_default: server_vad
    - id: gpt-4o-mini-realtime
      provider: openai
      input:
        sample_rate_hz: 24000
        mime_type: "audio/pcm;rate=24000"
      vad_default: server_vad
    # OpenAI Transcription Models  
    - id: gpt-4o-transcribe
      provider: openai
      input:
        sample_rate_hz: 24000
        mime_type: "audio/pcm;rate=24000"
      vad_default: server_vad
    - id: gpt-4o-mini-transcribe
      provider: openai
      input:
        sample_rate_hz: 24000
        mime_type: "audio/pcm;rate=24000"
      vad_default: server_vad
    - id: whisper-1
      provider: openai
      input:
        sample_rate_hz: 16000
        mime_type: "audio/pcm;rate=16000"
      vad_default: server_vad
    # Gemini Live Models
    - id: gemini-live-2.5-flash-preview
      provider: gemini
      input:
        sample_rate_hz: 16000
        mime_type: "audio/pcm;rate=16000"
      vad_default: server_vad
    - id: gemini-2.0-flash-live-001
      provider: gemini
      input:
        sample_rate_hz: 16000
        mime_type: "audio/pcm;rate=16000"
      vad_default: server_vad
  audio:
    max_buffer_ms: 5000
    chunk_target_ms: 50
    max_chunk_bytes: 262144  # 256KB
  vad:
    server_vad:
      interrupt_response: true
      silence_duration_ms: 500
      prefix_padding_ms: 50
    semantic_vad:
      interrupt_response: true
      eagerness: auto
  security:
    allow_client_ephemeral_tokens: true
    max_session_minutes: 15
    max_idle_seconds: 3600
  limits:
    max_sessions_per_api_key: 5
    max_concurrent_sessions: 100
    rpm_per_api_key: 120
    apm_audio_seconds_per_min: 180